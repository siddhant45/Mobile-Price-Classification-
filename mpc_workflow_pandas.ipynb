#Pipelines
import pandas as pd
from xgboost import XGBRegressor

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error


pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)

DATA_FILE = 'C:/your path........./mobile-price-classification/train.csv'

#Import Data
df = pd.read_csv(DATA_FILE)
df.head()


#Separate Variables
X = df.drop('price_range', axis=1)
y = df.price_range
X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)

#Split Data types
categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and 
                        X_train_full[cname].dtype == "object"]

numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]

my_cols = categorical_cols + numerical_cols

X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
X_train.head()

#Create Transformers for the diferent types of data
numerical_transformer = SimpleImputer(strategy='constant')


categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
#Add these transformers to a Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])
#Define Model
model = RandomForestRegressor(n_estimators=100, random_state=0)

#Create Pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)])
#Run the Pipeline
pipeline.fit(X_train, y_train)

#OUTPUT----------###########

Pipeline(memory=None,
         steps=[('preprocessor',
                 ColumnTransformer(n_jobs=None, remainder='drop',
                                   sparse_threshold=0.3,
                                   transformer_weights=None,
                                   transformers=[('num',
                                                  SimpleImputer(add_indicator=False,
                                                                copy=True,
                                                                fill_value=None,
                                                                missing_values=nan,
                                                                strategy='constant',
                                                                verbose=0),
                                                  ['battery_power', 'blue',
                                                   'clock_speed', 'dual_sim',
                                                   'fc', 'four_g', 'int_memory',
                                                   'm_dep...
                 RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,
                                       criterion='mse', max_depth=None,
                                       max_features='auto', max_leaf_nodes=None,
                                       max_samples=None,
                                       min_impurity_decrease=0.0,
                                       min_impurity_split=None,
                                       min_samples_leaf=1, min_samples_split=2,
                                       min_weight_fraction_leaf=0.0,
                                       n_estimators=100, n_jobs=None,
                                       oob_score=False, random_state=0,
                                       verbose=0, warm_start=False))],
         verbose=False)
predictions = pipeline.predict(X_valid)
results = X_valid
results['predicted'] = predictions
results['actual'] = y_valid
results['diff'] = abs(results['predicted'] - results['actual'])
results.head(10)
                
#--------------------------################################------------------------#     

battery_power	blue	clock_speed	dual_sim	fc	four_g	int_memory	m_dep	mobile_wt	n_cores	pc	px_height	px_width	ram	sc_h	sc_w	talk_time	three_g	touch_screen	wifi	predicted	actual	diff
405	1454	1	0.5	1	1	0	34	0.7	83	4	3	250	1033	3419	7	5	5	1	1	0	2.98	3	0.02
1190	1092	1	0.5	1	10	0	11	0.5	167	3	14	468	571	737	14	4	11	0	1	0	0.00	0	0.00
1132	1524	1	1.8	1	0	0	10	0.6	174	4	1	154	550	2678	16	5	13	1	0	1	1.97	2	0.03
731	1807	1	2.1	0	2	0	49	0.8	125	1	10	337	1384	1906	17	13	13	0	1	1	1.69	2	0.31
1754	1086	1	1.7	1	0	1	43	0.2	111	6	1	56	1150	3285	11	5	17	1	1	0	2.70	2	0.70
1178	909	1	0.5	1	9	0	30	0.4	97	3	10	290	773	594	12	0	4	1	1	1	0.00	0	0.00
1533	642	1	0.5	0	0	1	38	0.8	86	5	10	887	1775	435	9	2	2	1	1	0	0.08	0	0.08
1303	888	0	2.6	1	2	1	33	0.4	198	2	17	327	1683	3407	12	1	20	1	0	0	2.68	3	0.32
1857	914	1	0.7	0	1	1	60	0.9	198	5	4	740	840	3736	14	8	5	1	0	0	2.84	3	0.16
18	1131	1	0.5	1	11	0	49	0.6	101
#---------------------------------------------------------------------------------#
#Get the Mean Absolute Error

                #The MAE is a value that tells us within what distance of the actual value our prediction will fall, in this case it means that the model will return a result within 0.17 of the actual result (in our case our expected values range between 1 and 3) on average

score = mean_absolute_error(y_valid, predictions)
print('MAE:', score)
#Output MAE: 0.171375
